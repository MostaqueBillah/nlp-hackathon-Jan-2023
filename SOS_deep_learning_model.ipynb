{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RefOmAryZn5C",
        "outputId": "6f838dd4-9881-417c-db7a-36ab49690cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (21.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (0.1.97)\n",
            "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers[sentencepiece]) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1wtsxoXZrGG",
        "outputId": "68cd92fa-2a54-4273-8514-c0ec83f61103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/csebuetnlp/normalizer\n",
            "  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-x870_tn7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-x870_tn7\n",
            "  Resolved https://github.com/csebuetnlp/normalizer to commit d80c3c484e1b80268f2b2dfaf7557fe65e34f321\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from normalizer==0.0.1) (2022.6.2)\n",
            "Requirement already satisfied: emoji==1.4.2 in /usr/local/lib/python3.8/dist-packages (from normalizer==0.0.1) (1.4.2)\n",
            "Requirement already satisfied: ftfy==6.0.3 in /usr/local/lib/python3.8/dist-packages (from normalizer==0.0.1) (6.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from ftfy==6.0.3->normalizer==0.0.1) (0.2.5)\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/csebuetnlp/normalizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6iWegerEFPC",
        "outputId": "cefe02d7-e5d1-40d6-9445-38bf6cec6074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraForTokenClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
            "- This IS expected if you are initializing ElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "from normalizer import normalize # pip install git+https://github.com/csebuetnlp/normalizer\n",
        "import torch\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"csebuetnlp/banglabert\", num_labels=13) # num_labels will be the total no differents of NER tags ...\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/banglabert\", use_fast=True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lRCVoMSFrXAn"
      },
      "outputs": [],
      "source": [
        "model.config.architectures = ['ElectraForTokenClassification']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp=open(\"train.txt\", \"r\").read().split(\"\\n\\n\") #train data\n",
        "total_sentence=len(fp)\n",
        "# fn=open(\"newtrain.txt\",\"a\")\n",
        "# count=1\n",
        "x=[]\n",
        "y=[]\n",
        "d=[] # a unique list to store all unique class\n",
        "for l in fp:\n",
        "  word=\"\"\n",
        "  tag=\"\"\n",
        "  for i in l.split(\"\\n\"):\n",
        "    w=i.split(\" _ _ \")[0]\n",
        "    t=i.split(\" _ _ \")[1]\n",
        "\n",
        "    word+=w+\" \"\n",
        "    tag+=t+\" \"\n",
        "\n",
        "    if t not in d:\n",
        "      d.append(t)\n",
        "\n",
        "\n",
        "    # fn.write(s)\n",
        "  # count+=1\n",
        "  word=word[:len(word)-1]\n",
        "  tag=tag[:len(word)-1]\n",
        "  x.append(word)\n",
        "  y.append(tag.split())\n",
        "unique_tag_num=len(d)\n",
        "print(x[0])\n",
        "print(y[0])\n",
        "print(unique_tag_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ebHLUZVgeGH",
        "outputId": "c44d5ec1-7f96-4bbb-85c9-be91c70bf68b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "তার মৃত্যুর দশ দিন পর, ১১৫ কৃষ্ণাঙ্গ উচ্চ বিদ্যালয়ের শিক্ষার্থীরা তার হত্যার প্রতিবাদে ম্যাককম্ব এর মাধ্যমে মিছিল করেছে।\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O']\n",
            "13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH9dT9ZbO9N6",
        "outputId": "c29d6955-d869-4937-d1ee-4aac0f2a47c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'O': 0, 'B-LOC': 1, 'B-GRP': 2, 'I-GRP': 3, 'B-PROD': 4, 'B-CW': 5, 'I-CW': 6, 'B-CORP': 7, 'B-PER': 8, 'I-PER': 9, 'I-CORP': 10, 'I-PROD': 11, 'I-LOC': 12}\n",
            "{0: 'O', 1: 'B-LOC', 2: 'B-GRP', 3: 'I-GRP', 4: 'B-PROD', 5: 'B-CW', 6: 'I-CW', 7: 'B-CORP', 8: 'B-PER', 9: 'I-PER', 10: 'I-CORP', 11: 'I-PROD', 12: 'I-LOC'}\n"
          ]
        }
      ],
      "source": [
        "labels_to_ids = {k: v for v, k in enumerate(d)}\n",
        "ids_to_labels = {v: k for v, k in enumerate(d)}\n",
        "print(labels_to_ids)\n",
        "print(ids_to_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bGYwIJYBJT9Z"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# these functions are heavily influenced by the HF squad_metrics.py script\n",
        "def normalize_text(s):\n",
        "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "    import string, re\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        return re.sub(regex, \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def compute_exact_match(prediction, truth):\n",
        "    return int(normalize_text(prediction) == normalize_text(truth))\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "    pred_tokens = normalize_text(prediction).split()\n",
        "    truth_tokens = normalize_text(truth).split()\n",
        "    \n",
        "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "        return int(pred_tokens == truth_tokens)\n",
        "    \n",
        "    common_tokens = Counter(pred_tokens) & Counter(truth_tokens)\n",
        "    common_tokens = sum(common_tokens.values())\n",
        "    \n",
        "    # if there are no common tokens then f1 = 0\n",
        "    if common_tokens == 0:\n",
        "        return 0\n",
        "    \n",
        "    prec = 1.0 * common_tokens / len(pred_tokens)\n",
        "    rec = 1.0 * common_tokens / len(truth_tokens)\n",
        "    \n",
        "    return 2 * (prec * rec) / (prec + rec)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TMf0j3lHM6xZ"
      },
      "outputs": [],
      "source": [
        "label_all_tokens = False\n",
        "\n",
        "def align_label(qc, labels):\n",
        "    \n",
        "    tokenized_inputs = tokenizer(qc, max_length=128, padding=\"max_length\", truncation=True, return_attention_mask=True, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    \n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    #print(word_ids)\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "        #print(word_idx)\n",
        "        if word_idx is None:\n",
        "          label_ids.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "          try:\n",
        "            label_ids.append(labels_to_ids[labels[word_idx]])\n",
        "          except:\n",
        "            label_ids.append(-100)\n",
        "        else:\n",
        "          try:\n",
        "            label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
        "          except:\n",
        "            label_ids.append(-100)\n",
        "        \n",
        "        #print(label_ids)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return label_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = [normalize_text('তিনি বর্তমানে জাপানের সাংবিধানিক গণতান্ত্রিক দল এর সদস্য।'), normalize_text('তিনি যুবক হিসেবে শেফিল্ড বুধবার এফ.সি. যোগদান করেন এবং ১৯৫১ সালে তাদের পেশাগত দিক দিয়ে আত্মপ্রকাশ করেন।')]\n",
        "# y = ['O O B-GRP I-GRP I-GRP I-GRP O O'.split(), 'O O O B-GRP I-GRP I-GRP O O O O O O O O O O O'.split()]"
      ],
      "metadata": {
        "id": "i0XxOLAUBX5e"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(x)):\n",
        "  x[i]=normalize_text(x[i])"
      ],
      "metadata": {
        "id": "COOeCX7wliMb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train,test,train_tags,test_tags=train_test_split(x,y,test_size=0.1,train_size=0.9)\n",
        "\n",
        "print(len(train))\n",
        "print(len(test))\n",
        "print(len(train_tags))\n",
        "print(len(test_tags))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEaHlYeMly5g",
        "outputId": "2510919f-f5ff-4851-d557-de904c2eb8a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13770\n",
            "1530\n",
            "13770\n",
            "1530\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "o0h4mB5Rp6G5"
      },
      "outputs": [],
      "source": [
        "ins = []\n",
        "outs = []\n",
        "for i in range(len(train)):\n",
        "  ins.append(tokenizer(train[i], max_length=128, padding=\"max_length\", truncation=True, return_attention_mask=True, add_special_tokens=True, return_tensors=\"pt\"))\n",
        "  outs.append(align_label(train[i], train_tags[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6T4bc5Z7lFRc"
      },
      "outputs": [],
      "source": [
        "class DataSequence(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, i, o):\n",
        "\n",
        "        #lb = [i.split() for i in a]\n",
        "        self.texts = i\n",
        "        self.labels = o\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_data(self, idx):\n",
        "\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "\n",
        "        return torch.LongTensor(self.labels[idx])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_data = self.get_batch_data(idx)\n",
        "        batch_labels = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_data, batch_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yOdNy-NBr94G"
      },
      "outputs": [],
      "source": [
        "train_dataset = DataSequence(ins, outs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MlWJTcSdwEX3"
      },
      "outputs": [],
      "source": [
        "def align_word_ids(qc):\n",
        "  \n",
        "    tokenized_inputs = tokenizer(qc, max_length=128, padding=\"max_length\", truncation=True, return_attention_mask=True, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    #print(qc)\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "    #print(word_ids)\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "\n",
        "        elif word_idx != previous_word_idx:\n",
        "            try:\n",
        "                label_ids.append(1)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        else:\n",
        "            try:\n",
        "                label_ids.append(1 if label_all_tokens else -100)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return label_ids\n",
        "\n",
        "\n",
        "def evaluate_one_text(qc):\n",
        "\n",
        "    text = tokenizer(qc, max_length=512, padding=\"max_length\", truncation=True, return_attention_mask=True, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    \n",
        "    mask = text['attention_mask'].to(device)\n",
        "    input_id = text['input_ids'].to(device)\n",
        "    label_ids = torch.Tensor(align_word_ids(qc)).unsqueeze(0).to(device)\n",
        "    #print(label_ids)\n",
        "\n",
        "    logits = model(input_id, mask, None)\n",
        "    #print(logits[0])\n",
        "    #print(len(logits[0][0]))\n",
        "    #logits_clean = logits[0][label_ids != -100]\n",
        "    #print(logits_clean)\n",
        "    #print(logits[0][0])\n",
        "\n",
        "    predictions = logits[0][0].argmax(dim=1).tolist()\n",
        "    #print(predictions)\n",
        "    prediction_label = [ids_to_labels[i] for i in predictions]\n",
        "    #print(prediction_label)\n",
        "    #print(prediction_label)\n",
        "\n",
        "    #print(input_id)\n",
        "    a = tokenizer.convert_ids_to_tokens(input_id[0])\n",
        "    #print(a)\n",
        "    '''\n",
        "    print(prediction_label)\n",
        "    print(len(prediction_label))\n",
        "    print(a)\n",
        "    print(len(a))\n",
        "    '''\n",
        "\n",
        "    ans = ''\n",
        "    ind = True\n",
        "    for i in range(len(label_ids[0])):\n",
        "      if (label_ids[0][i] != -100):\n",
        "        ans = ans + ' ' + prediction_label[i]\n",
        "\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95FLsxPiJcP_",
        "outputId": "dc4533d5-ec0e-42ed-c7bc-bf795e9be41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 1722/1722 [05:38<00:00,  5.09it/s, loss=0.39]\n",
            "Epoch 1: 100%|██████████| 1722/1722 [05:33<00:00,  5.17it/s, loss=0.194]\n",
            "Epoch 2: 100%|██████████| 1722/1722 [05:32<00:00,  5.17it/s, loss=0.00782]\n",
            "Epoch 3: 100%|██████████| 1722/1722 [05:33<00:00,  5.17it/s, loss=0.0478]\n",
            "Epoch 4: 100%|██████████| 1722/1722 [05:32<00:00,  5.18it/s, loss=0.0237]\n",
            "Epoch 5: 100%|██████████| 1722/1722 [05:32<00:00,  5.18it/s, loss=0.017]\n",
            "Epoch 6: 100%|██████████| 1722/1722 [05:32<00:00,  5.18it/s, loss=0.00209]\n",
            "Epoch 7: 100%|██████████| 1722/1722 [05:32<00:00,  5.18it/s, loss=0.00135]\n",
            "Epoch 8: 100%|██████████| 1722/1722 [05:32<00:00,  5.18it/s, loss=0.000533]\n",
            "Epoch 9: 100%|██████████| 1722/1722 [05:32<00:00,  5.18it/s, loss=0.000656]\n",
            "Epoch 10: 100%|██████████| 1722/1722 [05:32<00:00,  5.18it/s, loss=0.000576]\n",
            "Epoch 11: 100%|██████████| 1722/1722 [05:32<00:00,  5.17it/s, loss=0.00196]\n",
            "Epoch 12: 100%|██████████| 1722/1722 [05:32<00:00,  5.18it/s, loss=0.000361]\n",
            "Epoch 13: 100%|██████████| 1722/1722 [05:32<00:00,  5.18it/s, loss=0.00305]\n",
            "Epoch 14: 100%|██████████| 1722/1722 [05:32<00:00,  5.18it/s, loss=0.000401]\n"
          ]
        }
      ],
      "source": [
        "# Training part \n",
        "# no of epoch = 15\n",
        "# opimizer = Adam\n",
        "# at every epoch EM and F1 scores were calculated on validation set of BanglaRQA\n",
        "# at every epoch model was also saved\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# setup GPU/CPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# move model over to detected device\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# activate training mode of model\n",
        "model.train()\n",
        "# initialize adam optimizer with weight decay (reduces chance of overfitting)\n",
        "optim = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# initialize data loader for training data\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "for epoch in range(15):\n",
        "    # set model to train mode\n",
        "    model.train()\n",
        "    model_name = 'banglaBERT_model_weights_epoch_' + str(epoch) + '.pth'\n",
        "    # setup loop (we use tqdm for the progress bar)\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    for train_data, train_label in loop:\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        \n",
        "        \n",
        "\n",
        "        optim.zero_grad()\n",
        "        # pull all the tensor batches required for training\n",
        "        input_ids = train_data['input_ids'].squeeze(1).to(device)\n",
        "        attention_mask = train_data['attention_mask'].squeeze(1).to(device)\n",
        "        labels = train_label.to(device)\n",
        "      \n",
        "        outputs = model(input_ids=input_ids, \n",
        "                          attention_mask=attention_mask, \n",
        "                            labels = labels)\n",
        "        #print(outputs)\n",
        "        # extract loss\n",
        "        loss = outputs[0]\n",
        "        #print(loss)\n",
        "        # calculate loss for every parameter that needs grad update \n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "\n",
        "        #print(outputs)\n",
        "        #print(model.config.architectures)\n",
        "        # print relevant info to progress bar\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    # torch.save({\n",
        "    #     'epoch': epoch,\n",
        "    #     'model_state_dict': model.state_dict(),\n",
        "    #     'optimizer_state_dict': optim.state_dict(),\n",
        "    #     'loss': loss,\n",
        "        \n",
        "    #     }, os.path.join(checkpoint_dir,model_name))\n",
        "    \n",
        "    optim.zero_grad()\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "    # f1_total = 0.0\n",
        "    # em_total = 0.0\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #   l = len(context_val)\n",
        "    #   for i in range(l):\n",
        "    #       pred = evaluate_one_text(question_val[i] + ' হ্যাঁ না । ' + context_val[i])\n",
        "    #       pred = pred.replace('#', '')\n",
        "    #       #if(i<10):\n",
        "    #         #print(pred)\n",
        "    #       f1_total = f1_total + compute_f1(pred, answer_val[i])\n",
        "    #       em_total = em_total + compute_exact_match(pred, answer_val[i])\n",
        "\n",
        "    #   print(\"Epoch\", epoch)\n",
        "    #   print('F1: ', f1_total/l)\n",
        "    #   print('EM: ', em_total/l)  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing with test data\n",
        "\n",
        "fp=open(\"test.txt\", \"r\").read().split(\"\\n\\n\") #train data\n",
        "\n",
        "x=[]\n",
        "\n",
        "for l in fp:\n",
        "  word=\"\"\n",
        "\n",
        "  for i in l.split():\n",
        "\n",
        "\n",
        "    word+=i+\" \"\n",
        "\n",
        "  word=word[:len(word)-1]\n",
        "  x.append(word)\n",
        "print(x[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsZhifWRdfPr",
        "outputId": "be9bd194-e6c8-4ae6-d782-de207d3d1dce"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "নিকটতম বিমানবন্দরটি ( ১৫৮ কিমি ) রাজীব গান্ধী আন্তর্জাতিক বিমানবন্দর ।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing with dev data\n",
        "predicted_file=open(\"dev_pred_labels.txt\",\"a\")\n",
        "\n",
        "with torch.no_grad():\n",
        "      l = len(x)\n",
        "      for i in range(l):\n",
        "        pred = evaluate_one_text(x[i])\n",
        "        pred = pred[:-1]\n",
        "        pred=pred.split()\n",
        "        # print(pred)\n",
        "        # print(len(x[i].split()),len(pred))\n",
        "        if len(x[i].split())>len(pred):\n",
        "          pred+=(['O']*(len(x[i].split())-len(pred)))\n",
        "        elif len(pred)>len(x[i].split()):\n",
        "          pred=pred[0:len(pred)-(len(pred)-len(x[i].split()))]\n",
        "        \n",
        "        s2=\"\"\n",
        "\n",
        "\n",
        "        for z in range(len(pred)):\n",
        "\n",
        "          s2+=pred[z]+\"\\n\"\n",
        "        s2+=\"\\n\"\n",
        "\n",
        "\n",
        "        predicted_file.write(s2)"
      ],
      "metadata": {
        "id": "09p2mxaOeIg2"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #testing with dev data\n",
        "\n",
        "# fp=open(\"dev.txt\", \"r\").read().split(\"\\n\\n\") #train data\n",
        "# total_sentence=len(fp)\n",
        "# # fn=open(\"newtrain.txt\",\"a\")\n",
        "# # count=1\n",
        "# x=[]\n",
        "# y=[]\n",
        "# for l in fp:\n",
        "#   word=\"\"\n",
        "#   tag=\"\"\n",
        "#   for i in l.split(\"\\n\"):\n",
        "#     w=i.split(\" _ _ \")[0]\n",
        "#     t=i.split(\" _ _ \")[1]\n",
        "\n",
        "#     word+=w+\" \"\n",
        "#     tag+=t+\" \"\n",
        "#   word=word[:len(word)-1]\n",
        "#   tag=tag[:len(word)-1]\n",
        "#   x.append(word)\n",
        "#   y.append(tag.split())\n",
        "# print(x[0])\n",
        "# print(y[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNd4KkMciWC9",
        "outputId": "217ecfa5-36de-496b-aabc-cc9ddc28f62a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "তিনি যুবক হিসেবে শেফিল্ড বুধবার এফ.সি. যোগদান করেন এবং ১৯৫১ সালে তাদের পেশাগত দিক দিয়ে আত্মপ্রকাশ করেন।\n",
            "['O', 'O', 'O', 'B-GRP', 'I-GRP', 'I-GRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #testing with dev data\n",
        "# label_file=open(\"dev_gt_labels.txt\",\"a\")\n",
        "# predicted_file=open(\"dev_pred_labels.txt\",\"a\")\n",
        "\n",
        "# with torch.no_grad():\n",
        "#       l = len(x)\n",
        "#       for i in range(l):\n",
        "#         pred = evaluate_one_text(x[i])\n",
        "#         pred = pred[:-1]\n",
        "#         pred=pred.split()\n",
        "#         if len(y[i])>len(pred):\n",
        "#           pred+=['O']*(len(y[i])-len(pred))\n",
        "#         elif len(pred)>len(y[i]):\n",
        "#           pred=pred[0:len(pred)-(len(pred)-len(y[i]))]\n",
        "\n",
        "#         s1=\"\"\n",
        "#         s2=\"\"\n",
        "\n",
        "\n",
        "#         for z in range(len(y[i])):\n",
        "#           if \"\\n\" in y[i][z] or \"\\n\" in pred[z]:\n",
        "#             print(\"seriously!!!!!\")\n",
        "#           s1+=(y[i][z]+\"\\n\")\n",
        "\n",
        "#           s2+=pred[z]+\"\\n\"\n",
        "#         s1+=\"\\n\"\n",
        "#         s2+=\"\\n\"\n",
        "#         if len(s1)==len(s2):\n",
        "          \n",
        "#           label_file.write(s1)\n",
        "#           predicted_file.write(s2)"
      ],
      "metadata": {
        "id": "nQfy9VhfjD50"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "YpIxpQXHrIIC"
      },
      "outputs": [],
      "source": [
        "# #testing with splitted train data\n",
        "# label_file=open(\"dev_gt_labels.txt\",\"a\")\n",
        "# predicted_file=open(\"dev_pred_labels.txt\",\"a\")\n",
        "\n",
        "# with torch.no_grad():\n",
        "#       l = len(test)\n",
        "#       for i in range(l):\n",
        "\n",
        "#         pred = evaluate_one_text(test[i])\n",
        "#         pred = pred[:-1]\n",
        "#         pred=pred.split()\n",
        "#         if len(test_tags[i])>len(pred):\n",
        "\n",
        "#           pred+=['O']*(len(test_tags[i])-len(pred))\n",
        "#         elif len(pred)>len(test_tags[i]):\n",
        "\n",
        "#           pred=pred[0:len(pred)-(len(pred)-len(test_tags[i]))]\n",
        "\n",
        "#         s1=\"\"\n",
        "#         s2=\"\"\n",
        "\n",
        "#         for z in range(len(test_tags[i])):\n",
        "#           if \"\\n\" in test_tags[i][z] or \"\\n\" in pred[z]:\n",
        "#             print(\"seriously!!!!!\")\n",
        "#           s1+=(test_tags[i][z]+\"\\n\")\n",
        "\n",
        "#           s2+=pred[z]+\"\\n\"\n",
        "#         s1+=\"\\n\"\n",
        "#         s2+=\"\\n\"\n",
        "#         if len(s1)==len(s2):\n",
        "          \n",
        "#           label_file.write(s1)\n",
        "#           predicted_file.write(s2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "label_file=open(\"dev_gt_labels.txt\",\"r\").readlines()\n",
        "predicted_file=open(\"dev_pred_labels.txt\",\"r\").readlines()\n",
        "print(len(label_file),len(predicted_file))\n",
        "\n"
      ],
      "metadata": {
        "id": "jDZzlArQQCHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "600061ca-faf7-4e6f-8aa1-b1233b070012"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "165656 164672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python eval_main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEZq7uaNGiFL",
        "outputId": "122a264f-895c-46b2-f1af-ad9f4aa797fd"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"P@LOC\": 0.0,\n",
            "  \"R@LOC\": 0.0,\n",
            "  \"F1@LOC\": 0.0,\n",
            "  \"P@PER\": 0.0,\n",
            "  \"R@PER\": 0.0,\n",
            "  \"F1@PER\": 0.0,\n",
            "  \"P@PRO\": 0.0,\n",
            "  \"R@PRO\": 0.0,\n",
            "  \"F1@PRO\": 0.0,\n",
            "  \"P@GR\": 0.0,\n",
            "  \"R@GR\": 0.0,\n",
            "  \"F1@GR\": 0.0,\n",
            "  \"P@PE\": 0.0,\n",
            "  \"R@PE\": 0.0,\n",
            "  \"F1@PE\": 0.0,\n",
            "  \"P@LO\": 0.0,\n",
            "  \"R@LO\": 0.0,\n",
            "  \"F1@LO\": 0.0,\n",
            "  \"P@COR\": 0.0,\n",
            "  \"R@COR\": 0.0,\n",
            "  \"F1@COR\": 0.0,\n",
            "  \"P@C\": 0.0,\n",
            "  \"R@C\": 0.0,\n",
            "  \"F1@C\": 0.0,\n",
            "  \"P@CW\": 0.0,\n",
            "  \"R@CW\": 0.0,\n",
            "  \"F1@CW\": 0.0,\n",
            "  \"P@CORP\": 0.0,\n",
            "  \"R@CORP\": 0.0,\n",
            "  \"F1@CORP\": 0.0,\n",
            "  \"P@PROD\": 0.0,\n",
            "  \"R@PROD\": 0.0,\n",
            "  \"F1@PROD\": 0.0,\n",
            "  \"P@GRP\": 0.0015218902927014864,\n",
            "  \"R@GRP\": 0.15153494448073154,\n",
            "  \"F1@GRP\": 0.0030135153565865234,\n",
            "  \"Precision\": 0.0015218902927014864,\n",
            "  \"Recall\": 0.01791644142404819,\n",
            "  \"F1\": 0.0028054730910243783,\n",
            "  \"MD@R\": 0.34388755888485595,\n",
            "  \"MD@P\": 0.02921110979913672,\n",
            "  \"MD@F1\": 0.053848153768933094,\n",
            "  \"ALLTRUE\": 12949,\n",
            "  \"ALLRECALLED\": 4453,\n",
            "  \"ALLPRED\": 152442\n",
            "}\n",
            "Precision 0.0, Recall 0.02, F1 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yvKhpWuIO5zX"
      },
      "execution_count": 38,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}